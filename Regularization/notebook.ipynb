{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c15446a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Regularization : \n",
    "# set of technique used in ML to prevent overfitting model to bestfit model \n",
    "# reduce the complexity of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fc742a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to penalize the terms ,not become very high (those terms discouraging overly complex models)\n",
    "# Initially there is no constraint on the values of constants of higher order terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3ee52631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint optimization \n",
    "# Add the lambda times of sum of squares of constants to loss function\n",
    "# As The new loss function has greater rss because of constraint\n",
    "# The OLS model will try to decrease the RSS by reducing the contributions of each weight (the already high ones will be little lower but the smaller weights will start tending to zero..)\n",
    "# The model will now try to choose terms which have lower weights as well as lower loss too (maintaining a balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b8b8fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overfitted model will have loss tending to zero so now adding constraint to loss function will try to create such weights such that loss should be less making the weights lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "32a03247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7c40051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge (L-2 norm) -> Adding the constriant as sum of squares of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c6b03852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso (L-1 norm) -> Adding the constraint as absolute of weights\n",
    "# In this the weights could be zero too that's why we can use this to do feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ab516874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# L1 vs L2 REGULARIZATION COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Aspect                 | L1 (Lasso)             | L2 (Ridge)             |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Penalty Function       | λ * Σ|wj|              | λ * Σwj²               |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Constraint Shape       | Diamond/Square          | Circle/Sphere          |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Sparsity              | Yes (exact zeros)       | No (shrinks to zero)   |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Feature Selection     | Automatic               | Manual                 |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Solution Method       | Iterative              | Closed-form            |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Computational Cost    | Higher                 | Lower                  |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Handles Correlated    | Picks one arbitrarily  | Distributes weights    |\n",
    "# | Features              |                        |                        |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Stability             | Less stable            | More stable            |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "# | Best Use Case         | Feature selection      | Weight shrinkage       |\n",
    "# |                       | High dimensions        | Multicollinearity      |\n",
    "# +------------------------+-------------------------+-------------------------+\n",
    "\n",
    "# =============================================================================\n",
    "# GEOMETRIC SHAPES (CONCEPTUAL)\n",
    "# =============================================================================\n",
    "\n",
    "# L1 CONSTRAINT REGION (DIAMOND/SQUARE)\n",
    "# ◊ Sharp corners and edges\n",
    "# ◊ Flat sides meet at 90-degree angles  \n",
    "# ◊ Corner points lie exactly on coordinate axes\n",
    "# ◊ High probability of solution landing on corners (where weights = 0)\n",
    "\n",
    "# L2 CONSTRAINT REGION (CIRCLE/SPHERE)\n",
    "# ○ Smooth, curved boundary\n",
    "# ○ No sharp corners or edges\n",
    "# ○ Never touches coordinate axes exactly\n",
    "# ○ Low probability of weights being exactly zero\n",
    "\n",
    "# =============================================================================\n",
    "# MATHEMATICAL FORMULATION\n",
    "# =============================================================================\n",
    "\n",
    "# LASSO (L1) LOSS FUNCTION:\n",
    "# L_lasso(w) = (1/2n) * Σ(yi - xi^T w)² + λ * Σ|wj|\n",
    "#              ↑_________________↑        ↑________↑\n",
    "#                   RSS term           L1 penalty\n",
    "\n",
    "# RIDGE (L2) LOSS FUNCTION:  \n",
    "# L_ridge(w) = (1/2n) * Σ(yi - xi^T w)² + λ * Σwj²\n",
    "#              ↑_________________↑        ↑_______↑\n",
    "#                   RSS term           L2 penalty\n",
    "\n",
    "# =============================================================================\n",
    "# KEY BEHAVIORAL DIFFERENCES\n",
    "# =============================================================================\n",
    "\n",
    "# L1 (LASSO) BEHAVIOR:\n",
    "# - Creates sparse solutions (many weights = 0)\n",
    "# - Performs automatic feature selection\n",
    "# - Can completely eliminate irrelevant features\n",
    "# - Sharp penalty creates \"all-or-nothing\" effect\n",
    "\n",
    "# L2 (RIDGE) BEHAVIOR:\n",
    "# - Shrinks all weights proportionally\n",
    "# - Keeps all features in the model\n",
    "# - Weights approach but never reach exactly zero\n",
    "# - Smooth penalty creates gradual shrinkage\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL IMPACT ON COEFFICIENTS\n",
    "# =============================================================================\n",
    "\n",
    "# ORIGINAL WEIGHTS (NO REGULARIZATION):\n",
    "# weights = [5.2, 12.8, -0.3, 0.1, -15.6, 2.1, 0.05, -8.4]\n",
    "\n",
    "# AFTER L1 (LASSO) REGULARIZATION:  \n",
    "# weights = [4.1, 10.2, 0.0, 0.0, -12.3, 1.8, 0.0, -6.2]  \n",
    "# ↑ Some weights become exactly 0 (feature selection)\n",
    "\n",
    "# AFTER L2 (RIDGE) REGULARIZATION:\n",
    "# weights = [3.8, 9.1, -0.2, 0.08, -11.2, 1.5, 0.03, -6.8]  \n",
    "# ↑ All weights shrunk but remain non-zero\n",
    "\n",
    "# =============================================================================\n",
    "# WHEN TO CHOOSE WHICH\n",
    "# =============================================================================\n",
    "\n",
    "# CHOOSE L1 (LASSO) WHEN:\n",
    "# - You have many features but expect only few are important\n",
    "# - You want automatic feature selection\n",
    "# - Interpretability is crucial (need sparse model)\n",
    "# - High-dimensional data (more features than samples)\n",
    "# - Examples: Gene selection, text classification, image processing\n",
    "\n",
    "# CHOOSE L2 (RIDGE) WHEN:\n",
    "# - All features might contribute somewhat to the prediction\n",
    "# - You have multicollinearity problems\n",
    "# - You want stable, robust predictions\n",
    "# - Computational efficiency is important\n",
    "# - Examples: Time series forecasting, financial modeling, sensor data analysis\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTION ALGORITHMS\n",
    "# =============================================================================\n",
    "\n",
    "# L1 (LASSO) SOLUTION:\n",
    "# - No closed-form solution\n",
    "# - Requires iterative methods:\n",
    "#   * Coordinate descent\n",
    "#   * Proximal gradient methods\n",
    "#   * Subgradient methods\n",
    "# - Soft thresholding: w_new = sign(w) * max(0, |w| - λ)\n",
    "\n",
    "# L2 (RIDGE) SOLUTION:\n",
    "# - Has closed-form solution:\n",
    "#   w_ridge = (X^T X + λI)^(-1) X^T y\n",
    "# - Can be computed directly using matrix operations\n",
    "# - Much faster computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7b8eda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GEOMETRIC VISUALIZATION (ASCII ART)\n",
    "# =============================================================================\n",
    "\n",
    "# L1 CONSTRAINT (DIAMOND SHAPE) - 2D Case\n",
    "# Constraint: |w1| + |w2| ≤ t\n",
    "#\n",
    "#         w2\n",
    "#          |\n",
    "#          |    /\\\n",
    "#          |   /  \\\n",
    "#      ----+--/----\\----w1\n",
    "#          | /      \\\n",
    "#          |/        \\\n",
    "#          +----------\n",
    "#         /|          \\\n",
    "#        / |           \\\n",
    "#       \\  |           /\n",
    "#        \\ |          /\n",
    "#         \\|         /\n",
    "#          +--------\n",
    "#          |\\      /\n",
    "#          | \\    /\n",
    "#          |  \\  /\n",
    "#          |   \\/\n",
    "#          |\n",
    "# Sharp corners at axes → High probability of w=0\n",
    "\n",
    "# L2 CONSTRAINT (CIRCLE SHAPE) - 2D Case  \n",
    "# Constraint: w1² + w2² ≤ t\n",
    "#\n",
    "#         w2\n",
    "#          |\n",
    "#          |   ****\n",
    "#          | **    **\n",
    "#      ----+*--------*----w1\n",
    "#          |*        *\n",
    "#          | **    **\n",
    "#          |   ****\n",
    "#          |\n",
    "# Smooth curve → Low probability of exactly hitting w=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9dda026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33:291-297, 1997.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data\n",
    "Y = housing.target\n",
    "features = housing.feature_names\n",
    "description = housing.DESCR\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a7a9c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test, Y_train , Y_test = train_test_split(X , Y , train_size=0.7 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5f656409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2065c1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957681720649695"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train ,Y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "r2_score(Y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "32629b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.49233996e-01,  1.22130962e-01, -2.99521134e-01,  3.48377554e-01,\n",
       "       -8.85561779e-04, -4.16992130e-02, -8.93880954e-01, -8.68628225e-01])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "782b2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5304834843385456\n",
      "0.5957700127249194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "ridge = Ridge(alpha=0.1) # alpha decides the lambda in theory higher alpha higher penalty\n",
    "ridge.fit(X_train ,Y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(mean_squared_error(Y_test , y_pred=y_pred))\n",
    "print(r2_score(Y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "782436c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.49227048e-01,  1.22139889e-01, -2.99496212e-01,  3.48346828e-01,\n",
       "       -8.82524162e-04, -4.16996542e-02, -8.93795621e-01, -8.68541682e-01])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "88e38230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9355843191720238\n",
      "0.2870819759728527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "lasso = Lasso(alpha=0.5) # alpha decides the lambda in theory higher alpha higher penalty\n",
    "lasso.fit(X_train ,Y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "print(mean_squared_error(Y_test , y_pred=y_pred))\n",
    "print(r2_score(Y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "246782cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimal alpha: 6.2506\n",
      "   Best CV R²: 0.6066\n",
      "\n",
      "📊 Final Performance:\n",
      "   Train R²: 0.6093\n",
      "   Test R²:  0.5959\n",
      "   Test MSE: 0.5303\n",
      "   Overfitting gap: 0.0135\n",
      "\n",
      "🔍 Model Complexity:\n",
      "   Max coefficient: 0.8886\n",
      "   Number of features: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Scale features (Ridge is scale-sensitive!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Use RidgeCV to find optimal alpha\n",
    "alphas = np.logspace(-3, 3, 50)  # 50 values from 0.001 to 1000\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# 3. Results\n",
    "print(f\"✅ Optimal alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"   Best CV R²: {ridge_cv.best_score_:.4f}\")\n",
    "\n",
    "# 4. Final evaluation\n",
    "y_pred = ridge_cv.predict(X_test_scaled)\n",
    "train_r2 = ridge_cv.score(X_train_scaled, Y_train)\n",
    "test_r2 = r2_score(Y_test, y_pred)\n",
    "test_mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "print(f\"\\n📊 Final Performance:\")\n",
    "print(f\"   Train R²: {train_r2:.4f}\")\n",
    "print(f\"   Test R²:  {test_r2:.4f}\")\n",
    "print(f\"   Test MSE: {test_mse:.4f}\")\n",
    "print(f\"   Overfitting gap: {train_r2 - test_r2:.4f}\")\n",
    "\n",
    "# 5. Check coefficients\n",
    "print(f\"\\n🔍 Model Complexity:\")\n",
    "print(f\"   Max coefficient: {np.max(np.abs(ridge_cv.coef_)):.4f}\")\n",
    "print(f\"   Number of features: {len(ridge_cv.coef_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6780ae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29662158,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
